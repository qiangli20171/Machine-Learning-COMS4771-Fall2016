# -*- coding: utf-8 -*-
"""
Python Code for HW1 Problem1

Created on Wed Sep 14 22:11:29 2016
@author: Yizhou Wang

"""

import numpy as np
import random
import matplotlib.pyplot as plt
import pylab as pl
from scipy import log 
from scipy.optimize import curve_fit
from scipy.io import loadmat

ocr = loadmat('ocr.mat')

#import matplotlib.pyplot as plt
#from matplotlib import cm
#plt.imshow((ocr['data'][0].reshape(28,28)), cmap = cm.gray_r)
#plt.show()

data = ocr['data'].astype(float)
labels = ocr['labels'].astype(float)
testdata = ocr['testdata'].astype(float)
testlabels = ocr['testlabels'].astype(float)




"""
function preds = onennc( X, Y, test )
1-nearest neighbor classifier with Euclidean distance
    
    This function implements the 1-nearest neighbor classifier with
    Euclidean distance. 

    Input:
        X   matrix of training feature vectors
        Y   vector of the corresponding labels of X
        test    matrix of test featrue vectors

    Output:
        preds   vector of predicted labels

@author: Yizhou Wang

"""
def onennc(X,Y,test):
    
    sn,ss = X.shape
    tn,ss = test.shape
    
    testT = test.transpose()    
    XtestT = np.dot(X,testT)
    
    sqX =  X * X
    sumsqX = np.matrix(np.sum(sqX, axis=1))
    sumsqXex = np.tile(sumsqX.transpose(), (1, tn))   
    
    sqedist = sumsqXex - 2*XtestT
    dist = sqedist
            
    indexmin = np.argmin(dist,axis=0)
    
    return Y[indexmin]
    
"""
END of onennc
"""






n = [1000, 2000, 4000, 8000]
error = np.zeros((4,10))

for i in range(0,4):
    for time in range(0,10):
        sel = random.sample(xrange(60000),n[i])
        preds = onennc(data[sel,:],labels[sel],testdata)
        error[i,time] = np.sum((testlabels!=preds).astype(float))/10000.0
        print error[i,time]

error_ave = np.mean(error,axis=1)
print error_ave
error_std = np.std(error,axis=1)
print error_std





"""
Draw a plot of test points and learning curve
The Learning Curve is generated by logistic fit
"""

"""
function para = polyfit( x, y )
Logistic fit using the input points.

    Input:
        x   the x-axis of input points
        y   the y-axis of input points
        
    Output:
        para    the parameters of LOG function a, b 
                a, b is discribed in function func
                
@author: Yizhou Wang
                
"""

def func(x, a, b):
    y = a * log(x) + b
    return y

def polyfit(x, y):
    popt, pcov = curve_fit(func, x, y)
    para = popt
    return para

"""
END of polyfit
"""

plt.errorbar(n, error_ave, yerr=error_std, fmt='o',label='test points')

lx = np.arange(0,60000,100)
para = polyfit(n,error_ave)
ly = para[0] * log(lx) + para[1]
plt.plot(lx,ly,label='learning curve')

pl.title('Learning Curve Plot')
pl.xlabel('Number of Training Data n')
pl.ylabel('Test Error Rate')
pl.xlim(0.0,60000.)
pl.ylim(0.0,0.15)

plt.legend()
plt.show()


            
    





